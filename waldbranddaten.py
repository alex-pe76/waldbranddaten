import pandas as pd
import json
import re
from rapidfuzz import process

bundeslaender = {
    "BW": "Baden-W√ºrttemberg",
    "BY": "Bayern",
    "BE": "Berlin",
    "BB": "Brandenburg",
    "HB": "Bremen",
    "HH": "Hamburg",
    "HE": "Hessen",
    "MV": "Mecklenburg-Vorpommern",
    "NI": "Niedersachsen",
    "NW": "Nordrhein-Westfalen",
    "RP": "Rheinland-Pfalz",
    "SL": "Saarland",
    "SN": "Sachsen",
    "ST": "Sachsen-Anhalt",
    "SH": "Schleswig-Holstein",
    "TH": "Th√ºringen"
}

base_url = "https://www.dwd.de/DWD/warnungen/agrar/wbx/wbx_tab_alle_{kuerzel}.html"

for kuerzel, name in bundeslaender.items():
    url = base_url.format(kuerzel=kuerzel)
    try:
        dfs = pd.read_html(url)
        print(f"‚úÖ {name} ({kuerzel}): {len(dfs)} Tabellen gefunden")
        if len(dfs) >= 2:
            df = dfs[1]
            # Filtere die Legenden-Zeile heraus (die eine Wiederholung der Spaltennamen ist)
            df = df[df[df.columns[0]] != df.columns[0]]
            df_json = df.to_dict(orient="records")
            with open(f"waldbrand_{kuerzel}.json", "w", encoding="utf-8") as f:
                json.dump(df_json, f, ensure_ascii=False, indent=2)
            print(f"üíæ Datei gespeichert: waldbrand_{kuerzel}.json")
        else:
            print(f"‚ö†Ô∏è {name}: Zu wenige Tabellen")
    except Exception as e:
        print(f"‚ùå Fehler bei {name} ({kuerzel}): {e}")


# Alle einzelnen JSON-Dateien zusammenf√ºhren
import os

gesamt_daten = []

for kuerzel in bundeslaender.keys():
    dateiname = f"waldbrand_{kuerzel}.json"
    if os.path.exists(dateiname):
        with open(dateiname, encoding="utf-8") as f:
            eintraege = json.load(f)
            for eintrag in eintraege:
                eintrag["Bundesland"] = bundeslaender[kuerzel]
            gesamt_daten.extend(eintraege)

# Nach dem Einlesen von gesamt_daten: Doppelte normalisierte Stationsnamen identifizieren
from collections import Counter

def normalize_name(name):
    name = name.upper()
    name = name.replace("√Ñ", "AE").replace("√ñ", "OE").replace("√ú", "UE")
    name = name.replace("√§", "AE").replace("√∂", "OE").replace("√º", "UE")
    name = name.replace("√ü", "SS")
    name = re.sub(r"[^\w\s]", " ", name)  # Entferne Sonderzeichen wie . und - und ersetze sie durch Leerzeichen
    name = re.sub(r"\s+", " ", name)  # Mehrfache Leerzeichen zusammenfassen
    return name.strip()

counter = Counter([normalize_name(e.get("Stationsname", "")) for e in gesamt_daten])
mehrfach_namen = {k: v for k, v in counter.items() if v > 1}
print(f"üëÄ Mehrfache Stationsnamen (normalisiert): {mehrfach_namen}")

# Gesamtdatei schreiben
with open("waldbrand_gesamt.json", "w", encoding="utf-8") as f:
    json.dump(gesamt_daten, f, ensure_ascii=False, indent=2)

print("‚úÖ Gesamtdatei gespeichert: waldbrand_gesamt.json")

 # Geokoordinaten erg√§nzen auf Basis von mosmix_stationskatalog1.txt
print("üìç Versuche Geokoordinaten √ºber mosmix_stationskatalog1.txt zu erg√§nzen...")
try:
    df_mosmix = pd.read_csv("mosmix_stationskatalog1.txt", sep="\t", encoding="utf-8", usecols=["NAME", "LAT", "LON"])
    df_mosmix = df_mosmix.rename(columns={"NAME": "Station", "LAT": "Latitude", "LON": "Longitude"})
    df_mosmix["Latitude"] = pd.to_numeric(df_mosmix["Latitude"].astype(str).str.replace(",", "."), errors="coerce")
    df_mosmix["Longitude"] = pd.to_numeric(df_mosmix["Longitude"].astype(str).str.replace(",", "."), errors="coerce")

    df_mosmix["norm_station"] = df_mosmix["Station"].apply(normalize_name)
    station_list = df_mosmix["norm_station"].tolist()

    # Neue Matching-Logik (siehe Aufgabenstellung)
    unmatched = []
    gefundene = 0

    # Vorbereitung f√ºr TF-IDF
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        tfidf_vectorizer = TfidfVectorizer().fit(station_list)
        station_matrix = tfidf_vectorizer.transform(station_list)
    except ImportError:
        tfidf_vectorizer = None
        station_matrix = None

    for eintrag in gesamt_daten:
        station_raw = eintrag.get("Stationsname", "")
        norm_name = normalize_name(station_raw)
        tokens = norm_name.split()
        match = pd.DataFrame()

        # Versuche exakten Match mit jedem Token
        for token in tokens:
            temp_match = df_mosmix[df_mosmix["norm_station"] == token]
            if temp_match.shape[0] == 1:
                match = temp_match
                break

        # Wenn kein direkter Match, pr√ºfe, ob alle Tokens in einem norm_station-Eintrag enthalten sind
        if match.shape[0] != 1:
            token_set = set(tokens)
            for _, row in df_mosmix.iterrows():
                station_tokens = set(row["norm_station"].split())
                if token_set.issubset(station_tokens):
                    match = pd.DataFrame([row])
                    break

        if match.shape[0] == 1:
            m = match.iloc[0]
            eintrag["Latitude"] = float(m["Latitude"])
            eintrag["Longitude"] = float(m["Longitude"])
            gefundene += 1
            print(f"üîç Match: '{station_raw}' ‚Üí '{m['norm_station']}' ‚Üí Koordinaten: ({m['Latitude']}, {m['Longitude']})")
        else:
            # Direkt TF-IDF Matching f√ºr alle √ºbrigen F√§lle
            if tfidf_vectorizer is not None and station_matrix is not None:
                query_vec = tfidf_vectorizer.transform([norm_name])
                similarities = cosine_similarity(query_vec, station_matrix).flatten()
                best_idx = similarities.argmax()
                best_score = similarities[best_idx]
                if best_score > 0.4:
                    best_match_name = station_list[best_idx]
                    match_row = df_mosmix[df_mosmix["norm_station"] == best_match_name].iloc[0]
                    eintrag["Latitude"] = float(match_row["Latitude"])
                    eintrag["Longitude"] = float(match_row["Longitude"])
                    gefundene += 1
                    print(f"üìê TF-IDF-Match: '{station_raw}' ‚Üí '{best_match_name}' ‚Üí Koordinaten: ({match_row['Latitude']}, {match_row['Longitude']})")
                else:
                    # F√ºge Vorschlagsliste hinzu
                    top_indices = similarities.argsort()[-3:][::-1]
                    vorschlaege = []
                    for idx in top_indices:
                        kandidat = station_list[idx]
                        score = similarities[idx]
                        vorschlaege.append({"name": kandidat, "score": round(float(score), 3)})
                    eintrag["Vorschlaege"] = vorschlaege
                    unmatched.append(station_raw)
                    print(f"‚ùì Kein sicherer Match f√ºr '{station_raw}'. Vorschl√§ge: {vorschlaege}")
            else:
                unmatched.append(station_raw)

    print(f"‚úÖ Koordinaten erg√§nzt f√ºr {gefundene} Stationen.")
    print(f"‚ÑπÔ∏è Keine √úbereinstimmung f√ºr {len(unmatched)} Stationen, z.‚ÄØB.: {unmatched[:5]}")
except Exception as e:
    print(f"‚ùå Fehler beim Einlesen der MOSMIX-Daten: {e}")

# Datei mit Koordinaten speichern
with open("waldbrand_gesamt.json", "w", encoding="utf-8") as f:
    for eintrag in gesamt_daten:
        if "Latitude" not in eintrag or eintrag["Latitude"] is None:
            eintrag["Latitude"] = 0.0
        if "Longitude" not in eintrag or eintrag["Longitude"] is None:
            eintrag["Longitude"] = 0.0
    json.dump(gesamt_daten, f, ensure_ascii=False, indent=2)
print("‚úÖ Gesamtdatei mit Koordinaten gespeichert: waldbrand_gesamt.json")
